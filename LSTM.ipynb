{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdmSCsACSnRv"
      },
      "source": [
        "!rm -rf waymo-od > /dev/null\n",
        "!git clone https://github.com/waymo-research/waymo-open-dataset.git waymo-od\n",
        "!cd waymo-od && git branch -a\n",
        "!cd waymo-od && git checkout remotes/origin/master\n",
        "!pip3 install --upgrade pip\n",
        "!pip3 install waymo-open-dataset-tf-2-1-0==1.2.0 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCH5ty84Sv3N"
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGrGOXKvSyTK"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LcOOUgQvS2ks"
      },
      "source": [
        "!echo \"deb http://packages.cloud.google.com/apt gcsfuse-bionic main\" > /etc/apt/sources.list.d/gcsfuse.list\n",
        "!curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -\n",
        "!apt -qq update\n",
        "!apt -qq install gcsfuse\n",
        "\n",
        "!mkdir -p data/training\n",
        "!gcsfuse --only-dir training/ waymo_open_dataset_v_1_2_0_individual_files data/training/\n",
        "\n",
        "!mkdir -p data/testing\n",
        "!gcsfuse --only-dir testing/ waymo_open_dataset_v_1_2_0_individual_files data/testing/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v831KtvVS9De"
      },
      "source": [
        "import os\n",
        "import gc\n",
        "import tensorflow as tf\n",
        "import math\n",
        "import numpy as np\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import files\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.utils.data as data_utils\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from waymo_open_dataset.utils import range_image_utils\n",
        "from waymo_open_dataset.utils import transform_utils\n",
        "from waymo_open_dataset.utils import  frame_utils\n",
        "from waymo_open_dataset import dataset_pb2 as open_dataset\n",
        "\n",
        "from Net_Lib import *\n",
        "\n",
        "list_train = os.listdir(path='data/training')\n",
        "list_test = os.listdir(path='data/testing')\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TZVvO6PNV5T"
      },
      "source": [
        "# LSTM Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqrsCpiiTGUq"
      },
      "source": [
        "## Batch-Maker"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdxakWRyNbIm"
      },
      "source": [
        "# Data extraction and preparation\n",
        "def create_LSTMbatch(index,train):\n",
        "  if(train):\n",
        "    loc = \"data/training/\"\n",
        "    req_list = list_train\n",
        "  else:\n",
        "    loc = \"data/testing/\"\n",
        "    req_list = list_test\n",
        "\n",
        "  train_images = []\n",
        "  train_labels = []\n",
        "  img_yes = False\n",
        "  imagenum = 0\n",
        "  for dataset in list_train[index:index+1]:\n",
        "    dataset = tf.data.TFRecordDataset(loc+dataset, compression_type='')\n",
        "    for data in dataset:\n",
        "      frame = open_dataset.Frame()\n",
        "      frame.ParseFromString(bytearray(data.numpy()))\n",
        "\n",
        "      # Get image itself\n",
        "      for index, image in enumerate(frame.images):\n",
        "        if(image.name == 1):\n",
        "          img = tf.image.decode_jpeg(image.image)\n",
        "          img = tf.image.resize(img, [640, 960])\n",
        "          train_images.append(img)\n",
        "\n",
        "      # Get label data\n",
        "      for cam_labels in frame.projected_lidar_labels:\n",
        "        if(cam_labels.name != 1):\n",
        "          continue\n",
        "        for label in cam_labels.labels:\n",
        "          train_labels.append(np.array([label.metadata.speed_x, label.metadata.speed_y, label.metadata.accel_x,\n",
        "                                            label.metadata.accel_y, imagenum]))\n",
        "          img_yes = True\n",
        "      \n",
        "      # Write specialized \"empty image\" output\n",
        "      if(not img_yes):\n",
        "        train_labels.append(np.array([-1,-1,-1,-1,imagenum]))\n",
        "      else:\n",
        "        img_yes = False\n",
        "      imagenum += 1\n",
        " \n",
        "  train_labels = torch.from_numpy(np.array(train_labels))\n",
        "  train_images = torch.from_numpy(np.array(train_images).transpose(0,3,1,2))\n",
        "  return train_labels, train_images"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3UOK5DSTI-J"
      },
      "source": [
        "## Small-Scale Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmqPSc0QTcpd"
      },
      "source": [
        "anchors = torch.load(\"anch.pt\")\n",
        "model_save_name = 'save_checkpoint.pt'\n",
        "path = F\"/content/gdrive/My Drive/{model_save_name}\" \n",
        "checkpoint = torch.load(path)\n",
        "\n",
        "n_tot_train = len(list_train)  # number of overall training examples\n",
        "\n",
        "nepoch_readbatch = 25          # number of epochs through training set\n",
        "batchsize = 16                 # minibatch size\n",
        "a_cuda = True                  # whether or not to enable cuda gpu acceleration\n",
        "\n",
        "res = 32                       # resolution of image breakdown\n",
        "H = 1280//(2*res)                   # height of the grid over images\n",
        "W = 1920//(2*res)                   # width of the grid over images\n",
        "\n",
        "network_spec = parse_cfg(\"yolo.cfg\")\n",
        "module_list = create_network(network_spec, 3)\n",
        "yolo = Model(module_list).float()\n",
        "yolo.load_state_dict(checkpoint['model_state_dict'])\n",
        "recurrent = LSTM_module(W*H*50, 360, 2, W*H*5)\n",
        "\n",
        "if(a_cuda):\n",
        "  yolo.cuda()\n",
        "  recurrent.cuda()\n",
        "\n",
        "lstm_loss = LSTM_loss(1,3,3,res,(H,W),a_cuda)\n",
        "losses = []\n",
        "\n",
        "# use ADAM optimizer\n",
        "optimizer = optim.Adam(recurrent.parameters(), lr=3E-2, weight_decay=0.0005)\n",
        "\n",
        "for i in [0]:\n",
        "  labels, images = create_LSTMbatch(i, True)\n",
        "  ntrain = images.shape[0]\n",
        "  # for loop to get from different batches of read-in images\n",
        "  for iepoch in range(nepoch_readbatch):\n",
        "    num_its = int(ntrain/batchsize)\n",
        "    ep_loss = np.zeros(num_its) \n",
        "    print(iepoch)\n",
        "    for t in range(num_its):\n",
        "        batchindices = np.random.choice(ntrain, batchsize, replace=False)\n",
        "\n",
        "        # before the forward pass, clean the gradient buffers of all parameters\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward pass\n",
        "        this_batch = Variable(images[batchindices,...].float())\n",
        "        if(a_cuda):\n",
        "          this_batch = this_batch.cuda()\n",
        "        yolo_out = yolo(this_batch)\n",
        "        yolo_out = nms_thresh(yolo_out, 0.65, anchors, a_cuda, res)\n",
        "        \n",
        "        lstm_input = shape_for_LSTM(yolo_out, a_cuda)\n",
        "        lstm_output = recurrent(lstm_input)\n",
        "        lstm_output = lstm_output.view(batchsize-1, 5, H, W)\n",
        "\n",
        "        # MSE loss\n",
        "        label_indices = (labels[:, -1][..., None] == torch.tensor(batchindices)).any(-1).nonzero().squeeze()\n",
        "        true_out = labels[label_indices,:]\n",
        "        if(a_cuda):\n",
        "          true_out = true_out.cuda()\n",
        "        \n",
        "        loss = lstm_loss(lstm_output, true_out)\n",
        "        ep_loss[t] = loss.item()\n",
        "\n",
        "        # backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # update parameters using SGD\n",
        "        optimizer.step()\n",
        "\n",
        "        # remove from memory\n",
        "        this_batch = None\n",
        "        true_out = None\n",
        "        yolo_out = None\n",
        "        lstm_input = None\n",
        "        lstm_output = None\n",
        "        gc.collect()\n",
        "        if(a_cuda):\n",
        "          torch.cuda.empty_cache()\n",
        "\n",
        "    losses.append(np.mean(ep_loss))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9v_HVAjewPYO"
      },
      "source": [
        "fig, ax = plt.subplots()\n",
        "ax.plot(losses)\n",
        "plt.xlabel('Epoch Number')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('LSTM Loss on Waymo Subset')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6HJF1oZTnZG"
      },
      "source": [
        "## Full-Scale Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jeuFZVw4TP4e",
        "outputId": "16c73b7b-c342-4841-a55b-b5d14afc4d18"
      },
      "source": [
        "anchors = torch.load(\"anch.pt\")\n",
        "model_save_name = 'lstm_checkpoint.pt'\n",
        "path = F\"/content/gdrive/My Drive/{model_save_name}\" \n",
        "resume = True\n",
        "\n",
        "base_model_name = '1_fixed_checkpoint.pt'\n",
        "base_path = F\"/content/gdrive/My Drive/{base_model_name}\" \n",
        "check = torch.load(base_path)\n",
        "\n",
        "tot_train = torch.load(\"train_idx.pt\")  # number of overall training examples\n",
        "tot_valid = torch.load(\"valid_idx.pt\")\n",
        "n_tot_train = len(tot_train)\n",
        "\n",
        "nepoch_dataset = 20            # number of epochs through training set\n",
        "batchsize = 16                 # minibatch size\n",
        "a_cuda = True                  # whether or not to enable cuda gpu acceleration\n",
        "\n",
        "res = 32                       # resolution of image breakdown\n",
        "H = 1280//(2*res)                   # height of the grid over images\n",
        "W = 1920//(2*res)                   # width of the grid over images\n",
        "epoch = 0\n",
        "\n",
        "network_spec = parse_cfg(\"yolo.cfg\")\n",
        "module_list = create_network(network_spec, 3)\n",
        "yolo = Model(module_list).float()\n",
        "yolo.load_state_dict(check['model_state_dict'])\n",
        "recurrent = LSTM_module(W*H*50, 360, 2, W*H*5)\n",
        "\n",
        "if(a_cuda):\n",
        "  yolo = yolo.cuda()\n",
        "  recurrent = recurrent.cuda()\n",
        "\n",
        "lstm_loss = LSTM_loss(1,3,3,res,(H,W),a_cuda)\n",
        "train_losses = []\n",
        "valid_losses = []\n",
        "\n",
        "# use ADAM optimizer\n",
        "optimizer = optim.Adam(recurrent.parameters(), lr=3E-2, weight_decay=0.0005)\n",
        "\n",
        "# Code for resume\n",
        "if(resume):\n",
        "  checkpoint = torch.load(path)\n",
        "  recurrent.load_state_dict(checkpoint['model_state_dict'])\n",
        "  optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "  epoch = checkpoint['epoch']+1\n",
        "  train_losses = checkpoint['loss']\n",
        "  valid_losses = checkpoint['valid']\n",
        "\n",
        "num_images = 200*n_tot_train\n",
        "num_steps = num_images//batchsize\n",
        "\n",
        "for iepoch in range(epoch, nepoch_dataset):\n",
        "  ep_loss = np.zeros(num_steps)\n",
        "  print(\"Epoch:\" +str(iepoch))\n",
        "  # for loop to train through different batches of read-in images\n",
        "  for j in range(num_steps):\n",
        "    print(str(j) + \"/\" + str(num_steps))\n",
        "    index = int(np.random.choice(tot_train,1)[0])\n",
        "    labels, images = create_LSTMbatch(index, True)\n",
        "    ntrain = images.shape[0]\n",
        "    batchindices = np.random.choice(ntrain, batchsize, replace=False)\n",
        "\n",
        "    # before the forward pass, clean the gradient buffers of all parameters\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # forward pass\n",
        "    this_batch = Variable(images[batchindices,...].float())\n",
        "    if(a_cuda):\n",
        "      this_batch = this_batch.cuda()\n",
        "    with(torch.no_grad()):\n",
        "      yolo_out = yolo(this_batch)\n",
        "        \n",
        "    lstm_input = shape_for_LSTM(yolo_out, a_cuda)\n",
        "    lstm_output = recurrent(lstm_input)\n",
        "    lstm_output = lstm_output.view(batchsize-1, 5, H, W)\n",
        "\n",
        "    # MSE loss\n",
        "    label_indices = (labels[:, -1][..., None] == torch.tensor(batchindices)).any(-1).nonzero().squeeze()\n",
        "    true_out = labels[label_indices,:]\n",
        "    if(a_cuda):\n",
        "      true_out = true_out.cuda()\n",
        "        \n",
        "    loss = lstm_loss(lstm_output, true_out)\n",
        "    ep_loss[j] = loss.item()\n",
        "\n",
        "    # backward pass\n",
        "    loss.backward()\n",
        "\n",
        "    # update parameters using SGD\n",
        "    optimizer.step()\n",
        "\n",
        "    # remove from memory\n",
        "    this_batch = None\n",
        "    true_out = None\n",
        "    yolo_out = None\n",
        "    lstm_input = None\n",
        "    lstm_output = None\n",
        "    gc.collect()\n",
        "    if(a_cuda):\n",
        "      torch.cuda.empty_cache()\n",
        "\n",
        "  # Record training loss\n",
        "  train_losses.append(np.mean(ep_loss))\n",
        "\n",
        "  # Calculate and record validation loss using test set\n",
        "  valid = np.zeros(len(tot_valid))\n",
        "  for i in range(len(tot_valid)):\n",
        "    index = int(tot_valid[i])\n",
        "    labels, images = create_LSTMbatch(index, True)\n",
        "    ntest = images.shape[0]\n",
        "    batchindices = np.random.choice(ntest, batchsize, replace=False)\n",
        "    this_batch = Variable(images[batchindices,...].float())\n",
        "    if(a_cuda):\n",
        "      this_batch = this_batch.cuda()\n",
        "    yolo_out = yolo(this_batch)\n",
        "    lstm_input = shape_for_LSTM(yolo_out, a_cuda)\n",
        "    lstm_output = recurrent(lstm_input)\n",
        "    lstm_output = lstm_output.view(batchsize-1, 5, H, W)\n",
        "\n",
        "    label_indices = (labels[:, -1][..., None] == torch.tensor(batchindices)).any(-1).nonzero().squeeze()\n",
        "    true_out = labels[label_indices,:]\n",
        "    if(a_cuda):\n",
        "      true_out = true_out.cuda()\n",
        "    valid[i] = lstm_loss(lstm_output, true_out)\n",
        "\n",
        "    this_batch = None\n",
        "    true_out = None\n",
        "    yolo_out = None\n",
        "    lstm_input = None\n",
        "    lstm_output = None\n",
        "    gc.collect()\n",
        "    if(a_cuda):\n",
        "      torch.cuda.empty_cache()\n",
        "  valid_losses.append(np.mean(valid))\n",
        "\n",
        "  torch.save({\n",
        "            'epoch': iepoch,\n",
        "            'model_state_dict': recurrent.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': train_losses,\n",
        "            'valid': valid_losses\n",
        "            }, path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:2\n",
            "0/125\n",
            "1/125\n",
            "2/125\n",
            "3/125\n",
            "4/125\n",
            "5/125\n",
            "6/125\n",
            "7/125\n",
            "8/125\n",
            "9/125\n",
            "10/125\n",
            "11/125\n",
            "12/125\n",
            "13/125\n",
            "14/125\n",
            "15/125\n",
            "16/125\n",
            "17/125\n",
            "18/125\n",
            "19/125\n",
            "20/125\n",
            "21/125\n",
            "22/125\n",
            "23/125\n",
            "24/125\n",
            "25/125\n",
            "26/125\n",
            "27/125\n",
            "28/125\n",
            "29/125\n",
            "30/125\n",
            "31/125\n",
            "32/125\n",
            "33/125\n",
            "34/125\n",
            "35/125\n",
            "36/125\n",
            "37/125\n",
            "38/125\n",
            "39/125\n",
            "40/125\n",
            "41/125\n",
            "42/125\n",
            "43/125\n",
            "44/125\n",
            "45/125\n",
            "46/125\n",
            "47/125\n",
            "48/125\n",
            "49/125\n",
            "50/125\n",
            "51/125\n",
            "52/125\n",
            "53/125\n",
            "54/125\n",
            "55/125\n",
            "56/125\n",
            "57/125\n",
            "58/125\n",
            "59/125\n",
            "60/125\n",
            "61/125\n",
            "62/125\n",
            "63/125\n",
            "64/125\n",
            "65/125\n",
            "66/125\n",
            "67/125\n",
            "68/125\n",
            "69/125\n",
            "70/125\n",
            "71/125\n",
            "72/125\n",
            "73/125\n",
            "74/125\n",
            "75/125\n",
            "76/125\n",
            "77/125\n",
            "78/125\n",
            "79/125\n",
            "80/125\n",
            "81/125\n",
            "82/125\n",
            "83/125\n",
            "84/125\n",
            "85/125\n",
            "86/125\n",
            "87/125\n",
            "88/125\n",
            "89/125\n",
            "90/125\n",
            "91/125\n",
            "92/125\n",
            "93/125\n",
            "94/125\n",
            "95/125\n",
            "96/125\n",
            "97/125\n",
            "98/125\n",
            "99/125\n",
            "100/125\n",
            "101/125\n",
            "102/125\n",
            "103/125\n",
            "104/125\n",
            "105/125\n",
            "106/125\n",
            "107/125\n",
            "108/125\n",
            "109/125\n",
            "110/125\n",
            "111/125\n",
            "112/125\n",
            "113/125\n",
            "114/125\n",
            "115/125\n",
            "116/125\n",
            "117/125\n",
            "118/125\n",
            "119/125\n",
            "120/125\n",
            "121/125\n",
            "122/125\n",
            "123/125\n",
            "124/125\n",
            "Epoch:3\n",
            "0/125\n",
            "1/125\n",
            "2/125\n",
            "3/125\n",
            "4/125\n",
            "5/125\n",
            "6/125\n",
            "7/125\n",
            "8/125\n",
            "9/125\n",
            "10/125\n",
            "11/125\n",
            "12/125\n",
            "13/125\n",
            "14/125\n",
            "15/125\n",
            "16/125\n",
            "17/125\n",
            "18/125\n",
            "19/125\n",
            "20/125\n",
            "21/125\n",
            "22/125\n",
            "23/125\n",
            "24/125\n",
            "25/125\n",
            "26/125\n",
            "27/125\n",
            "28/125\n",
            "29/125\n",
            "30/125\n",
            "31/125\n",
            "32/125\n",
            "33/125\n",
            "34/125\n",
            "35/125\n",
            "36/125\n",
            "37/125\n",
            "38/125\n",
            "39/125\n",
            "40/125\n",
            "41/125\n",
            "42/125\n",
            "43/125\n",
            "44/125\n",
            "45/125\n",
            "46/125\n",
            "47/125\n",
            "48/125\n",
            "49/125\n",
            "50/125\n",
            "51/125\n",
            "52/125\n",
            "53/125\n",
            "54/125\n",
            "55/125\n",
            "56/125\n",
            "57/125\n",
            "58/125\n",
            "59/125\n",
            "60/125\n",
            "61/125\n",
            "62/125\n",
            "63/125\n",
            "64/125\n",
            "65/125\n",
            "66/125\n",
            "67/125\n",
            "68/125\n",
            "69/125\n",
            "70/125\n",
            "71/125\n",
            "72/125\n",
            "73/125\n",
            "74/125\n",
            "75/125\n",
            "76/125\n",
            "77/125\n",
            "78/125\n",
            "79/125\n",
            "80/125\n",
            "81/125\n",
            "82/125\n",
            "83/125\n",
            "84/125\n",
            "85/125\n",
            "86/125\n",
            "87/125\n",
            "88/125\n",
            "89/125\n",
            "90/125\n",
            "91/125\n",
            "92/125\n",
            "93/125\n",
            "94/125\n",
            "95/125\n",
            "96/125\n",
            "97/125\n",
            "98/125\n",
            "99/125\n",
            "100/125\n",
            "101/125\n",
            "102/125\n",
            "103/125\n",
            "104/125\n",
            "105/125\n",
            "106/125\n",
            "107/125\n",
            "108/125\n",
            "109/125\n",
            "110/125\n",
            "111/125\n",
            "112/125\n",
            "113/125\n",
            "114/125\n",
            "115/125\n",
            "116/125\n",
            "117/125\n",
            "118/125\n",
            "119/125\n",
            "120/125\n",
            "121/125\n",
            "122/125\n",
            "123/125\n",
            "124/125\n",
            "Epoch:4\n",
            "0/125\n",
            "1/125\n",
            "2/125\n",
            "3/125\n",
            "4/125\n",
            "5/125\n",
            "6/125\n",
            "7/125\n",
            "8/125\n",
            "9/125\n",
            "10/125\n",
            "11/125\n",
            "12/125\n",
            "13/125\n",
            "14/125\n",
            "15/125\n",
            "16/125\n",
            "17/125\n",
            "18/125\n",
            "19/125\n",
            "20/125\n",
            "21/125\n",
            "22/125\n",
            "23/125\n",
            "24/125\n",
            "25/125\n",
            "26/125\n",
            "27/125\n",
            "28/125\n",
            "29/125\n",
            "30/125\n",
            "31/125\n",
            "32/125\n",
            "33/125\n",
            "34/125\n",
            "35/125\n",
            "36/125\n",
            "37/125\n",
            "38/125\n",
            "39/125\n",
            "40/125\n",
            "41/125\n",
            "42/125\n",
            "43/125\n",
            "44/125\n",
            "45/125\n",
            "46/125\n",
            "47/125\n",
            "48/125\n",
            "49/125\n",
            "50/125\n",
            "51/125\n",
            "52/125\n",
            "53/125\n",
            "54/125\n",
            "55/125\n",
            "56/125\n",
            "57/125\n",
            "58/125\n",
            "59/125\n",
            "60/125\n",
            "61/125\n",
            "62/125\n",
            "63/125\n",
            "64/125\n",
            "65/125\n",
            "66/125\n",
            "67/125\n",
            "68/125\n",
            "69/125\n",
            "70/125\n",
            "71/125\n",
            "72/125\n",
            "73/125\n",
            "74/125\n",
            "75/125\n",
            "76/125\n",
            "77/125\n",
            "78/125\n",
            "79/125\n",
            "80/125\n",
            "81/125\n",
            "82/125\n",
            "83/125\n",
            "84/125\n",
            "85/125\n",
            "86/125\n",
            "87/125\n",
            "88/125\n",
            "89/125\n",
            "90/125\n",
            "91/125\n",
            "92/125\n",
            "93/125\n",
            "94/125\n",
            "95/125\n",
            "96/125\n",
            "97/125\n",
            "98/125\n",
            "99/125\n",
            "100/125\n",
            "101/125\n",
            "102/125\n",
            "103/125\n",
            "104/125\n",
            "105/125\n",
            "106/125\n",
            "107/125\n",
            "108/125\n",
            "109/125\n",
            "110/125\n",
            "111/125\n",
            "112/125\n",
            "113/125\n",
            "114/125\n",
            "115/125\n",
            "116/125\n",
            "117/125\n",
            "118/125\n",
            "119/125\n",
            "120/125\n",
            "121/125\n",
            "122/125\n",
            "123/125\n",
            "124/125\n",
            "Epoch:5\n",
            "0/125\n",
            "1/125\n",
            "2/125\n",
            "3/125\n",
            "4/125\n",
            "5/125\n",
            "6/125\n",
            "7/125\n",
            "8/125\n",
            "9/125\n",
            "10/125\n",
            "11/125\n",
            "12/125\n",
            "13/125\n",
            "14/125\n",
            "15/125\n",
            "16/125\n",
            "17/125\n",
            "18/125\n",
            "19/125\n",
            "20/125\n",
            "21/125\n",
            "22/125\n",
            "23/125\n",
            "24/125\n",
            "25/125\n",
            "26/125\n",
            "27/125\n",
            "28/125\n",
            "29/125\n",
            "30/125\n",
            "31/125\n",
            "32/125\n",
            "33/125\n",
            "34/125\n",
            "35/125\n",
            "36/125\n",
            "37/125\n",
            "38/125\n",
            "39/125\n",
            "40/125\n",
            "41/125\n",
            "42/125\n",
            "43/125\n",
            "44/125\n",
            "45/125\n",
            "46/125\n",
            "47/125\n",
            "48/125\n",
            "49/125\n",
            "50/125\n",
            "51/125\n",
            "52/125\n",
            "53/125\n",
            "54/125\n",
            "55/125\n",
            "56/125\n",
            "57/125\n",
            "58/125\n",
            "59/125\n",
            "60/125\n",
            "61/125\n",
            "62/125\n",
            "63/125\n",
            "64/125\n",
            "65/125\n",
            "66/125\n",
            "67/125\n",
            "68/125\n",
            "69/125\n",
            "70/125\n",
            "71/125\n",
            "72/125\n",
            "73/125\n",
            "74/125\n",
            "75/125\n",
            "76/125\n",
            "77/125\n",
            "78/125\n",
            "79/125\n",
            "80/125\n",
            "81/125\n",
            "82/125\n",
            "83/125\n",
            "84/125\n",
            "85/125\n",
            "86/125\n",
            "87/125\n",
            "88/125\n",
            "89/125\n",
            "90/125\n",
            "91/125\n",
            "92/125\n",
            "93/125\n",
            "94/125\n",
            "95/125\n",
            "96/125\n",
            "97/125\n",
            "98/125\n",
            "99/125\n",
            "100/125\n",
            "101/125\n",
            "102/125\n",
            "103/125\n",
            "104/125\n",
            "105/125\n",
            "106/125\n",
            "107/125\n",
            "108/125\n",
            "109/125\n",
            "110/125\n",
            "111/125\n",
            "112/125\n",
            "113/125\n",
            "114/125\n",
            "115/125\n",
            "116/125\n",
            "117/125\n",
            "118/125\n",
            "119/125\n",
            "120/125\n",
            "121/125\n",
            "122/125\n",
            "123/125\n",
            "124/125\n",
            "Epoch:6\n",
            "0/125\n",
            "1/125\n",
            "2/125\n",
            "3/125\n",
            "4/125\n",
            "5/125\n",
            "6/125\n",
            "7/125\n",
            "8/125\n",
            "9/125\n",
            "10/125\n",
            "11/125\n",
            "12/125\n",
            "13/125\n",
            "14/125\n",
            "15/125\n",
            "16/125\n",
            "17/125\n",
            "18/125\n",
            "19/125\n",
            "20/125\n",
            "21/125\n",
            "22/125\n",
            "23/125\n",
            "24/125\n",
            "25/125\n",
            "26/125\n",
            "27/125\n",
            "28/125\n",
            "29/125\n",
            "30/125\n",
            "31/125\n",
            "32/125\n",
            "33/125\n",
            "34/125\n",
            "35/125\n",
            "36/125\n",
            "37/125\n",
            "38/125\n",
            "39/125\n",
            "40/125\n",
            "41/125\n",
            "42/125\n",
            "43/125\n",
            "44/125\n",
            "45/125\n",
            "46/125\n",
            "47/125\n",
            "48/125\n",
            "49/125\n",
            "50/125\n",
            "51/125\n",
            "52/125\n",
            "53/125\n",
            "54/125\n",
            "55/125\n",
            "56/125\n",
            "57/125\n",
            "58/125\n",
            "59/125\n",
            "60/125\n",
            "61/125\n",
            "62/125\n",
            "63/125\n",
            "64/125\n",
            "65/125\n",
            "66/125\n",
            "67/125\n",
            "68/125\n",
            "69/125\n",
            "70/125\n",
            "71/125\n",
            "72/125\n",
            "73/125\n",
            "74/125\n",
            "75/125\n",
            "76/125\n",
            "77/125\n",
            "78/125\n",
            "79/125\n",
            "80/125\n",
            "81/125\n",
            "82/125\n",
            "83/125\n",
            "84/125\n",
            "85/125\n",
            "86/125\n",
            "87/125\n",
            "88/125\n",
            "89/125\n",
            "90/125\n",
            "91/125\n",
            "92/125\n",
            "93/125\n",
            "94/125\n",
            "95/125\n",
            "96/125\n",
            "97/125\n",
            "98/125\n",
            "99/125\n",
            "100/125\n",
            "101/125\n",
            "102/125\n",
            "103/125\n",
            "104/125\n",
            "105/125\n",
            "106/125\n",
            "107/125\n",
            "108/125\n",
            "109/125\n",
            "110/125\n",
            "111/125\n",
            "112/125\n",
            "113/125\n",
            "114/125\n",
            "115/125\n",
            "116/125\n",
            "117/125\n",
            "118/125\n",
            "119/125\n",
            "120/125\n",
            "121/125\n",
            "122/125\n",
            "123/125\n",
            "124/125\n",
            "Epoch:7\n",
            "0/125\n",
            "1/125\n",
            "2/125\n",
            "3/125\n",
            "4/125\n",
            "5/125\n",
            "6/125\n",
            "7/125\n",
            "8/125\n",
            "9/125\n",
            "10/125\n",
            "11/125\n",
            "12/125\n",
            "13/125\n",
            "14/125\n",
            "15/125\n",
            "16/125\n",
            "17/125\n",
            "18/125\n",
            "19/125\n",
            "20/125\n",
            "21/125\n",
            "22/125\n",
            "23/125\n",
            "24/125\n",
            "25/125\n",
            "26/125\n",
            "27/125\n",
            "28/125\n",
            "29/125\n",
            "30/125\n",
            "31/125\n",
            "32/125\n",
            "33/125\n",
            "34/125\n",
            "35/125\n",
            "36/125\n",
            "37/125\n",
            "38/125\n",
            "39/125\n",
            "40/125\n",
            "41/125\n",
            "42/125\n",
            "43/125\n",
            "44/125\n",
            "45/125\n",
            "46/125\n",
            "47/125\n",
            "48/125\n",
            "49/125\n",
            "50/125\n",
            "51/125\n",
            "52/125\n",
            "53/125\n",
            "54/125\n",
            "55/125\n",
            "56/125\n",
            "57/125\n",
            "58/125\n",
            "59/125\n",
            "60/125\n",
            "61/125\n",
            "62/125\n",
            "63/125\n",
            "64/125\n",
            "65/125\n",
            "66/125\n",
            "67/125\n",
            "68/125\n",
            "69/125\n",
            "70/125\n",
            "71/125\n",
            "72/125\n",
            "73/125\n",
            "74/125\n",
            "75/125\n",
            "76/125\n",
            "77/125\n",
            "78/125\n",
            "79/125\n",
            "80/125\n",
            "81/125\n",
            "82/125\n",
            "83/125\n",
            "84/125\n",
            "85/125\n",
            "86/125\n",
            "87/125\n",
            "88/125\n",
            "89/125\n",
            "90/125\n",
            "91/125\n",
            "92/125\n",
            "93/125\n",
            "94/125\n",
            "95/125\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}